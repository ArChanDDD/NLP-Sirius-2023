{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Большие языковые модели (используем google colab на максимум)\n",
        "\n",
        "![img](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4470ce74-e595-4750-92a5-5f21f040df6d_577x432.jpeg)"
      ],
      "metadata": {
        "id": "I3Y7OhU0MaF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 1. Инжиниринг подсказок (prompt engeneering)\n",
        "\n",
        "В задании мы будем использовать общедоступные API-интерфейсы, моделей у которых 100+ миллиардов весов для логического вывода. Ваша задача — подтолкнуть модель к решению нескольких задач за вас.\n",
        "\n",
        "- Если у вас есть доступ к ChatGPT от openAI используйте его [(чятикГПТ)](https://chat.openai.com/chat)\n",
        "\n",
        "- Если нет, используйте BLOOM API [(БЛУМ)](https://huggingface.co/bigscience/bloom)"
      ],
      "metadata": {
        "id": "ScE_fX8uNDg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 1.**\n",
        "\n",
        "Заставьте модель сгенерировать диалог двух персон на выбор:\n",
        "- Известная личность или политический деятель\n",
        "- Вымышленный персонаж (из литературы, кино и тд.)\n",
        "- Лично вы\n",
        "\n",
        "Сравните два случая: \n",
        "\n",
        "1) подсказка содержла только имена\n",
        "\n",
        "2) подсказка содержала дополнительную информацию (см. пример ниже)\n",
        "\n",
        "\n",
        "![img](https://i.imgur.com/a1QhKF7.png)"
      ],
      "metadata": {
        "id": "onAWbPyPOeq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 2.**\n",
        "\n",
        "Используйте zero-shot prompt (без примеров), чтобы перевести отрывок монолога гамлета с русского на английский.\n",
        "\n",
        "```\n",
        "To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "And by opposing end them. To die—to sleep,\n",
        "No more; and by a sleep to say we end\n",
        "The heart-ache and the thousand natural shocks\n",
        "That flesh is heir to: 'tis a consummation\n",
        "Devoutly to be wish'd. To die, to sleep;\n",
        "To sleep, perchance to dream—ay, there's the rub:\n",
        "For in that sleep of death what dreams may come,\n",
        "When we have shuffled off this mortal coil,\n",
        "Must give us pause—there's the respect\n",
        "That makes calamity of so long life.\n",
        "```"
      ],
      "metadata": {
        "id": "NpoZ2UoORyae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 3.**\n",
        "Создайте быстрые и короткие подсказки, которые заставят модель изменить родовые местоимения главного действующего лица в данном предложении в любом направлении по вашему выбору. Например: doctor took off his mask <-> the doctor took of her mask.\n"
      ],
      "metadata": {
        "id": "uvyaMe4jTozm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 4**\n",
        "\n",
        "Напишите подсказку и предоставьте примеры, чтобы модель преобразовывала имперские единицы в метрические единицы (мили -> километры; мили в час -> км/ч). Учтите что модель должна переводить значения а не только менять единицу измерения, т.е. 1 mile -> 1.6 km а не 1 mile -> 1 km (Модель не обязательно должна точно переводить, но хотя бы примерно правильно. Скажем с точностью до целых).\n",
        "\n",
        "После того, как он работает с базовыми расстояниями и скоростью, попробуйте найти сложные примеры, где он не работает."
      ],
      "metadata": {
        "id": "8K8y0dyyUYqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tunning больших языковых моделей.\n",
        "\n",
        "Коллаб предоставляет доступ к GPU, для этого нужно поменять runtime. \n",
        "<center><img src=\"https://i.imgur.com/OOfDYzJ.png\" width=240px></center>"
      ],
      "metadata": {
        "id": "QmiX3JDfVmZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь давайте попробуем загрузить уменьшенную версию OPT без API. Мы будем использовать OPT-6b7 с 6,7 млрд параметров. Осторожно: хотя эта модель меньше, чем модели в API, она все же более чем в 60 раз больше, чем BERT. Приведенный ниже код едва помещается в память, поэтому убедитесь, что у вас больше ничего не загружено. Иногда вам нужно перезапустить среду выполнения, чтобы код заработал."
      ],
      "metadata": {
        "id": "peZOa-DzWuV9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6BmEYD2MT7z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.get_device_capability() < (7, 5):\n",
        "  raise ValueError(f\"You got a GPU with capability {torch.cuda.get_device_capability()}, need at least (7, 5)\")\n",
        "else: print(\"OK\")\n",
        "\n",
        "# Note: this code requires a Turing GPU or newer. Good: T4, RTX 20xx/30xx, A100/Axx; Bad: K80, P100, V100\n",
        "# Colab gives you T4. If you get older GPUs, please wait or switch to a new account (don't use both at the same time)\n",
        "%pip install --quiet bitsandbytes==0.35.4 transformers==4.24.0 datasets==2.7.0 accelerate==0.14.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ниже приведено много кода. Чтобы в нем разобраться нужно пройти следющие курсы в университете по порядку: ML, DL, NLP. А для этих курсов желательно уже знать матанализ, теорвер и линейную алгебру. \n",
        "\n",
        "В общем просто запустите при желании этот код, чтобы посмотреть на что в пределе способем google colab. Модель займет всю оперативку и обучать на видеокарте её надо минимум 3 часа.\n",
        "\n",
        "Что происходит: мы взяли большую языковую модель, предобученную на твиттере. После этого мы её дообучаем на корпусе с [английскими цитатами](https://huggingface.co/datasets/Abirate/english_quotes). После этого модель сможет правдоподобно генерировать цитаты знаменитых людей"
      ],
      "metadata": {
        "id": "-wd3INciXbvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-6.7b\", load_in_8bit=True, device_map='auto',\n",
        "    low_cpu_mem_usage=True, torch_dtype=torch.float16, offload_state_dict=True)\n",
        "# note: these flags slow down the code to save RAM; remove them if you have >32GB RAM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\")\n",
        "\n",
        "for module in model.modules():\n",
        "    if isinstance(module, bnb.nn.Linear8bitLt):\n",
        "        module.state.memory_efficient_backward = True\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.model.decoder.project_in = lambda x: x.requires_grad_(True)\n",
        "\n",
        "# cast model outputs to float32 to unfuck the top-k sampler\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "old_lm_head = model.lm_head\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "2cO-5xGCWhXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer(\"Mark Zuckerberg is\", return_tensors='pt')\n",
        "# note to self: find a less controversial example\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, min_length=30, max_length=30, do_sample=True)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0].numpy()))"
      ],
      "metadata": {
        "id": "gC3DUCw8yLIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
        "    def __init__(self, module: nn.Linear, rank: int):\n",
        "        super().__init__()\n",
        "        self.module = module\n",
        "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
        "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
        "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.module(input) + torch.matmul(torch.matmul(input, self.adapter_A), self.adapter_B)\n"
      ],
      "metadata": {
        "id": "Tb9PmJ4RXA6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test your implementation\n",
        "test_linear = nn.Linear(128, 128)\n",
        "test_linear.weight.data[...] = torch.eye(128)\n",
        "test_adapter = LoRALayer(test_linear, rank=8)\n",
        "\n",
        "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n",
        "\n",
        "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n",
        "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n",
        "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n",
        "\n",
        "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128), torch.linspace(-1, 1, 128).unsqueeze(0))\n",
        "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n",
        "dummy_loss.backward()\n",
        "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n",
        "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n",
        "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n",
        "# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n",
        "del dummy_loss, test_linear, test_adapter\n",
        "print(\"All tests passed!\")"
      ],
      "metadata": {
        "id": "3s41BZJ3nYWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "  if 'OPTAttention' in repr(type(module)):\n",
        "    module.q_proj = LoRALayer(module.q_proj, rank=8)\n",
        "    module.k_proj = LoRALayer(module.k_proj, rank=8)\n",
        "    module.v_proj = LoRALayer(module.v_proj, rank=8)"
      ],
      "metadata": {
        "id": "DWHXObEJaPUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer(\"Mark Zuckerberg is\", return_tensors='pt')\n",
        "# test a single training step, make sure we get meaningful gradients\n",
        "with torch.cuda.amp.autocast():\n",
        "  out = model.forward(**batch)\n",
        "  out.logits.norm().backward()\n",
        "\n",
        "for module in model.modules():\n",
        "  if isinstance(module, LoRALayer):\n",
        "    assert module.adapter_B.grad is not None\n",
        "    assert module.adapter_B.grad.norm().item() > 0\n",
        "\n",
        "model.zero_grad(set_to_none=True)"
      ],
      "metadata": {
        "id": "RHKdB1GInUOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "data = load_dataset(\"Abirate/english_quotes\")\n",
        "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model, train_dataset=data['train'],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4, gradient_accumulation_steps=4,\n",
        "        warmup_steps=250, max_steps=500, learning_rate=2e-4, fp16=True,\n",
        "        logging_steps=1, output_dir='outputs'),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "HTLBUAYg3pP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на результат:"
      ],
      "metadata": {
        "id": "nmPdPtrM4biv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer(\"If you want to be rich\", return_tensors='pt')\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, min_length=30, max_length=200, do_sample=True)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0].numpy()))"
      ],
      "metadata": {
        "id": "lOM9IOzC4dPr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}