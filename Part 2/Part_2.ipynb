{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Language Models\n",
        "\n",
        "В этот раз поработаем с более наглядными и чуть более интуитивными методами - `Bag of words` и вероятностной моделью."
      ],
      "metadata": {
        "id": "0ZXgiyUBHWsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7o7-HaTHP6Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "with open('SpamDatas.txt', 'r') as f:\n",
        "  full_text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of words\n",
        "\n",
        "Суть метода похожа на `word2vec`, но с одним отличием: раньше мы сопоставляли каждому слову вектор какой-то длины, а теперь будем сопоставлять вектор целому предложению.\n",
        "\n",
        "Как правило, этим методом пользуются (пользовались, точнее), когда нужно быстро и интуитивно:\n",
        "* Проверить похожесть предложений\n",
        "* Классифицировать предложения (например, проверить на спам)\n",
        "* Любые другие задачи, которые требуют обращать внимание на клюбчевые слова"
      ],
      "metadata": {
        "id": "XsAdQtX0HmaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "На сегодня наша задача следующая: будем пытаться определить ботов.\n",
        "\n",
        "Для начала, давайте посмотрим на имеющиеся данные:"
      ],
      "metadata": {
        "id": "EUVY-N0GMH1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_text[:993])"
      ],
      "metadata": {
        "id": "GPXDJQidJi5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте представим все в более читабельном виде."
      ],
      "metadata": {
        "id": "R5kW8QuANWta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "lines = [x.split('\\t') for x in full_text.split('\\n')]\n",
        "\n",
        "df = pd.DataFrame(columns=['target', 'text'], data=lines)\n",
        "df"
      ],
      "metadata": {
        "id": "sQfsFsuMMgGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так-то лучше!\n",
        "\n",
        "Итого, у нас есть 5574 смс-сообщения, для каждого из которых указано, спам это (`spam`), или нет (`ham`).\n",
        "\n",
        "Как не странно, те, кто шлют спам - боты, именно их мы и будем стараться определить."
      ],
      "metadata": {
        "id": "BcrxKzL5NzV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Предложения, слова, снова предложения\n",
        "\n",
        "Самая первая задача - посчитать общее количество уникальных слов, встречающихся в этих сообщениях - именно такой длины будет вектор для каждого предложения. Порядок действий будет такой:\n",
        "1. Сначала, сообщения, конечно, нужно токенизировать - сложно смотреть на набор буквы, нам нужны слова! \n",
        " \n",
        " Для этого вновь воспользуемся **nltk**.\n",
        "\n",
        "2. Дальше, воспользуемся `Counter`'ом, чтобы узнать количество уникальных слов (или можно как-то еще?)\n",
        " \n",
        " **Не забудьте привести все к нижнему регистру, применить стемминг (`stemmer.stem()`) и убрать стоп-слова**\n",
        "\n",
        "3. Создадим `base_dict` следующего вида:\n",
        "\n",
        " * Ключи - найденные раньше уникальные слова\n",
        " * Значения - `0` (он же базовый все-таки)"
      ],
      "metadata": {
        "id": "juatqJBTOYbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1"
      ],
      "metadata": {
        "id": "fHmi9wIJIbIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "_ILAGGgS5Trx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stemmer = SnowballStemmer(\"english\") \n",
        "tokenizer = WordPunctTokenizer()"
      ],
      "metadata": {
        "id": "9jOENrxuNf0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2"
      ],
      "metadata": {
        "id": "zDO6qYCsIfk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n",
        "total_words = ..."
      ],
      "metadata": {
        "id": "ylCdBOG2Psuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_words"
      ],
      "metadata": {
        "id": "O6PR7Yak4NO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3"
      ],
      "metadata": {
        "id": "I3InwgrHIgri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n",
        "base_dict = ... "
      ],
      "metadata": {
        "id": "Jkj9GlcbIZiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert total_words == 7502, 'Неправильно посчитано количество слов'\n",
        "assert len(base_dict) == total_words, 'Вектор учитывает не все слова'\n",
        "assert sum(base_dict.values()) == 0, 'Значения `base_dict` должны быть нулями'"
      ],
      "metadata": {
        "id": "aEkn8tYWHMHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подсчет векторов для предложений\n",
        "\n",
        "Теперь необходимо сделать следующее: построить вектор параметров и вектор истинных ответов для обучения.\n",
        "\n",
        "Если с вектором истинных ответов есть лишь одна хотелка - сделать `spam -> 1`, `ham -> -1`, то с вектором параметров ситуация чуть сложнее, а именно:\n",
        "\n",
        "1. Необходимо для каждого текста скопировать `base_dict`.\n",
        "2. Далее, заполнить его так, чтобы у каждого `key` было `value`, соответствующее числу повторений этого `key` в текущем тексте.\n",
        "3. Также необходимо создать `target_vector` - вектор целей (`1` или `-1`) в том же порядке, в котором добавлялись словари в предыдущем пункте.\n",
        "3. Таким образом, должно получиться, что `i` элемент в `text_vectors` является словарем, и его классом является `i` элемент из `target_vector`"
      ],
      "metadata": {
        "id": "EAnEC1hVPM4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "target2val = {'ham': -1, 'spam': 1}\n",
        "# То есть 1 - это спам, а -1 - это нормальное сообщение\n",
        "\n",
        "# TODO\n",
        "\n",
        "text_vectors = ... \n",
        "target_vector = ..."
      ],
      "metadata": {
        "id": "X-4anrU4PMP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(text_vectors[0]) == total_words \\\n",
        "        and len(text_vectors) == len(target_vector), 'Неправильно составлены вектора,'"
      ],
      "metadata": {
        "id": "g60unwdLMBD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = [list(x.values()) for x in text_vectors]\n",
        "y = target_vector\n",
        "\n",
        "X_train = X[:5000]\n",
        "X_test = X[5000:]\n",
        "y_train = y[:5000]\n",
        "y_test = y[5000:]"
      ],
      "metadata": {
        "id": "8SRujjSn6kuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Предсказания\n",
        "\n",
        "Итак, дело за малым - давайте построим логистическую регрессию, чтобы классифицировать спам и Не-спам.\n",
        "\n",
        "p.s. если вы впервые слышите про логистическую регрессию - [тык](https://habr.com/ru/company/io/blog/265007/) сюда"
      ],
      "metadata": {
        "id": "RHvOAeTG757s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "32TtAVWy7vzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n",
        "model = ..."
      ],
      "metadata": {
        "id": "g4lijAwK8D7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "y5T-fb68Byy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь даже можно попробовать самому попридумывать спам, чтобы обмануть нашу, кажется, довольно не глупую модель."
      ],
      "metadata": {
        "id": "Jyzl4C7ZB0mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val2target = {-1 : 'ham', 1 : 'spam'}\n",
        "\n",
        "def text_to_answer(text):\n",
        "  cur_vector = base_dict.copy()\n",
        "  for tok in tokenizer.tokenize(text):\n",
        "    token = stemmer.stem(tok.lower())\n",
        "    if token in stop_words:\n",
        "      continue\n",
        "    try:\n",
        "      cur_vector[token] += 1\n",
        "    except:\n",
        "      print(token + ' not in vocabulary')\n",
        "\n",
        "  return val2target[model.predict([list(cur_vector.values())])[0]]"
      ],
      "metadata": {
        "id": "ggWZy8ZoAD3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_answer(\"You have won a 1 week FREE in SIRIUS! Txt the word: URA YA MOLODETS\")"
      ],
      "metadata": {
        "id": "KpeoO9mb-LDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_answer('Your teacher is very happy that you translated this sentence :)')"
      ],
      "metadata": {
        "id": "_IN9PiFL_HLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Тут можно поиграться. А можно и не играться..."
      ],
      "metadata": {
        "id": "KfXOnMyQCO5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-граммы\n",
        "\n",
        "Пришло время сделать маленький шаг в сторону темы курса - давайте попробуем что-то посочинять. К примеру, посочиняем спамовые сообщения!"
      ],
      "metadata": {
        "id": "s32uTkBOCsUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Модель N_gramm\n",
        "\n",
        "На самом деле, сама логика (опять же) довольно проста. Мы будем генерировать слово, обращая внимание на `n` предыдущих слов - мы будем генерировать слова исходя из формулы \n",
        "\n",
        "$$\n",
        "P(x_i \\mid x_{i-n}, \\dots, x_{i-1})\n",
        "$$\n",
        "\n",
        "Переведем ее на человеко-читаемый язык - если мы обучались на трех предложениях:\n",
        "\n",
        "* \"В детстве я любил гулять\"\n",
        "* \"В детстве я любил, когда мама забирала меня из детского сада пораньше\"\n",
        "* \"В детстве я любил преподавателей, которые не отправляли меня на пересдачу 4 раза в год\"\n",
        "\n",
        "тогда для `n=4`, `i=5`, у нас получается:\n",
        "\n",
        "* $P('гулять' \\mid 'В', 'детстве', 'я', 'любил') = \\frac{1}{3}$\n",
        "* $P(',' \\mid 'В', 'детстве', 'я', 'любил') = \\frac{1}{3}$\n",
        "* $P('преподавателей' \\mid 'В', 'детстве', 'я', 'любил') = \\frac{1}{3}$\n",
        "\n",
        "Почему так? Потому что в случае начала предложения со слова 'В', последующие 3 определятся однозначно, ведь так? И только на 5 слове у нас возникнет какая-то вероятность, и уже после этого мы с вероятностью $\\frac{1}{3}$ выдадим слово `гулять`, `,` или `преподавателей`.\n",
        "\n",
        "Абсолютно очевидно, что для генерации чего-то действительно интересного нам понадобится ооооочень большой датасет (хотя, он кажется всегда нужен)."
      ],
      "metadata": {
        "id": "Oj4Z9D2XHz1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для решения этого довольно интересного задания предлагаем вам реализовать целый класс!\n",
        "\n",
        "p.s. если вы не работали с классами, попросите преподавателя, он расскажет, что к чему :)"
      ],
      "metadata": {
        "id": "j2p40hPTK3qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class N_gramm:\n",
        "  def __init__(self, n=3):\n",
        "    # Метод инициализации класса - достаточно просто сохранить параметры\n",
        "    self.n = n\n",
        "  \n",
        "  def fit(self, sentences):\n",
        "    # Метод тренировки\n",
        "    # На вход поступает список из списка токенов\n",
        "    # Ваша задача - пройтись по этим спискам окном размера n и составить словарь token_probs:\n",
        "    #   Его ключи - n-граммы\n",
        "    #   Его значения - словари, у которых\n",
        "    #     Ключи - токены, следующие за соответствующей n-граммой\n",
        "    #     Значения - вероятность получения этого токена при условии соответствующей n-граммы\n",
        "    # Таким образом, для упомянутого выше примера получим\n",
        "    # token_probs['В детстве я любил']['гулять'] = 0.3333333333333333\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    token_probs = ...\n",
        "  \n",
        "  def predict(self, prefix):\n",
        "    # Метод по входному префиксу (тексту) предсказывает следующий токен.\n",
        "    assert len(tokenizer.tokenize(prefix)) >= self.n, 'Префиксы должен быть длины хотя бы {} токена'.format(self.n)\n",
        "    \n",
        "    ngram = tokenizer.tokenize(prefix)[-self.n:]\n",
        "    try:\n",
        "      tokens = self.token_probs[' '.join(ngram)]\n",
        "      next_token = np.random.choice(list(tokens.keys()), p=list(tokens.values()))\n",
        "      return next_token, prefix + ' ' + next_token\n",
        "    except:\n",
        "      return None, None\n",
        "  \n",
        "  def generate_text(self, start_text, max_length=100, stop_prob = 0.2):\n",
        "    # Используя метод predict вам необходимо реализовать метод generate_text, \n",
        "    # Который будет возвращать текст длины не больше чем max_length.\n",
        "    # Для вероятности преждевременного завершения нужно после генерации одного из знаков \n",
        "    # '.', '?', '!' с вероятностью stop_prob завершать генерацию.\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    generated_text = ...\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "SL9J30u3CMSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ng = N_gramm(n=4)\n",
        "\n",
        "ng.fit([\"В детстве я любил гулять\",\n",
        "        \"В детстве я любил, когда мама забирала меня из детского сада пораньше\",\n",
        "        \"В детстве я любил преподавателей, которые не отправляли меня на пересдачу 4 раза в год\"])\n",
        "\n",
        "assert len(ng.token_probs) == 20, 'Неправильно работает fit'\n",
        "assert np.isclose(np.mean([sum(x.values()) for x in ng.token_probs.values()]), 1), 'Вероятность продолжения каждой n-граммы долджна быть равна 1'"
      ],
      "metadata": {
        "id": "UJ-WL7ugLO_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подготовка датасета\n",
        "\n",
        "А теперь пора создать датасет.\n",
        "\n",
        "Поскольку мы хотим научиться создавать спам-письма, будет логично на них и обучиться.\n",
        "\n",
        "Задача следующая:\n",
        "\n",
        "* Выбрать все текста, у которых `target=spam`\n",
        "* Собрать их в один списоки и отправить обучаться в `N_gramm`."
      ],
      "metadata": {
        "id": "99RjGQKlb0rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n",
        "spam_texts = ..."
      ],
      "metadata": {
        "id": "d89uAeAJQsrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение спаму"
      ],
      "metadata": {
        "id": "SA_1eJheeAdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n",
        "NGramm = ..."
      ],
      "metadata": {
        "id": "SpjLkUSpTJym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NGramm.generate_text('Hey there')"
      ],
      "metadata": {
        "id": "RgZqAdUWd7uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Task\n",
        "\n",
        "Давайте на момент вспомним ту самую одноклассницу, которая прочитала все фанфики по Гарри Поттеру (мы уверены, у вас такая есть) и убедимся в том, что она прочитала НЕ ВСЕ.\n",
        "\n",
        "А именно - давайте сами его напишем! Точнее, попросим нашу прикольную модельку его написать. "
      ],
      "metadata": {
        "id": "1lAfxoYIe8bQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это бонусное задание, его делать совсем не обязательно. \n",
        "\n",
        "Ваша задача - обучить `N_gramm` на отрывке из Гарри Поттера из предыдущей домашки и попробовать за счет параметров получить какую-то +- осмысленную историю."
      ],
      "metadata": {
        "id": "FX8E2ZXffnTE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_hrhfRwe7uT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}